---
title: "Class 09 Mini-Project"
author: "Claire Chapman"
date: "10/27/2021"
output: pdf_document
---

## Preparing the Data
Reading in the data
```{r}
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names = 1)
```

Examine the data
```{r}
head(wisc.df)
```
Get rid of the "Diagnosis" column because we won't be needing it
```{r}
wisc.data <- wisc.df[,-1]
```

But store "Diagnosis" as a factor to be used later to check our work
```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
```

> Q1. How many observations are in this dataset?

```{r}
dim(wisc.data)
```
569 observations

> Q2. How many of the observations have a malignant diagnosis?

```{r}
table(diagnosis)
```

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
length(grep("_mean", colnames(wisc.data)))
```
## Principal Component Analysis
Checking column means and standard deviation
```{r}
colMeans(wisc.data)
apply(wisc.data, 2, sd)
```
Perform PCA
```{r}
wisc.pr <- prcomp(wisc.data, scale = T)
```

```{r}
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

From row 2 of the summary above, PC1 accounts for 44.27%

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

From row 3 of the summary above, 3 PCs are required to describe >70%

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

From row 3 of the summary above, 7 PCs are required to describe >90%

## Interpreting PCA Results
```{r}
biplot(wisc.pr)
```
> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

There is way too much information on this plot for it to be understood and to be useful.


Let's make a better plot
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis, xlab = "PC1", ylab = "PC2")
```
> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis, xlab = "PC1", ylab = "PC3")
```
### Using ggplot2 to make another plot
Turn the PCA into a dataframe, add back diagnosis as a column
```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
```
```{r}
library(ggplot2)
```

Make a scatter plot colored by diagnosis
```{r}
ggplot(df, aes(PC1, PC2, col = diagnosis)) +
  geom_point()
```

## Variance Explained
Calculate variance of each component
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```
Calculate the variance explained by each principal component
```{r}
pve <- pr.var / sum(pr.var)
```

Plot variance explained for each principal component
```{r}
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0,1), type = "o")
```

Alternative scree plot of the same data
```{r}
barplot(pve, ylab = "Precent of Variance Explained", names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

### Exploring additional CRAN packages
```{r}
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

## Communicating PCA Results
Loadings: vectors that explain the mapping from the original features to the PC

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation[,1]
```
The loading vector for concave.points_mean is -0.260

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
summary(wisc.pr)
```
You need 5 principal components to explain >80% of the variance

## Hierarchical Clustering
First scale the data
```{r}
data.scaled <- scale(wisc.data)
```

Calculate the distances between all pairs of observations
```{r}
data.dist <- dist(data.scaled)
```

Create hierarchical clustering model
```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```

## Results of HClustering
> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h = 19, col = "red", lty = 2)
```
There are four clusters at about height 19

## Selecting number of clusters
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, h = 19)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```
> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

```{r}
wisc.xclusters <- cutree(wisc.hclust, h = 13)
table(wisc.xclusters, diagnosis)
```
```{r}
wisc.xclusters <- cutree(wisc.hclust, h = 15)
table(wisc.xclusters, diagnosis)
```
```{r}
wisc.xclusters <- cutree(wisc.hclust, h = 18)
table(wisc.xclusters, diagnosis)
```

```{r}
wisc.xclusters <- cutree(wisc.hclust, h = 20)
table(wisc.xclusters, diagnosis)
```

```{r}
wisc.xclusters <- cutree(wisc.hclust, h = 22)
table(wisc.xclusters, diagnosis)
```
```{r}
wisc.xclusters <- cutree(wisc.hclust, h = 25)
table(wisc.xclusters, diagnosis)
```

4 cluster provided some of the best results but using 5 clusters, at h = 18, could be a better decision. The two main benign and malignant groups retain the same values as in the original 4 cluster model but now, the additional three clusters are at least 100% benign or 100% malignant. In the 4 cluster model, one of the additional clusters was a mix of benign (29%) and malignant (71%).

## Using Different Methods
> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

"complete" for reference
```{r}
plot(wisc.hclust)
```

"average" 
```{r}
plot(hclust(data.dist, method = "average"))
```

"single"
```{r}
plot(hclust(data.dist, method = "single"))
```
"ward.D2"
```{r}
plot(hclust(data.dist, method = "ward.D2"))
```
My favorite method was ward.D2 because it was the easiest to read. The symmetry of the plot made the larger clusters much easier to see. Additionally, all branches ended in the same place on a horizontal line which makes more conceptual sense to me.

## Combining Methods
Does PCA improve or degrade the performance of hierarchical clustering?
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method = "ward.D2")
plot(wisc.pr.hclust)
```

Are these two main branches representative of malignant and benign tumors?

# **TO BE CONT.**
