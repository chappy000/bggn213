---
title: "Class 09 Mini-Project Completed"
author: "Claire Chapman"
date: "10/29/2021"
output:
  html_document:
    df_print: paged
---

## Preparing the Data
Reading in the data
```{r}
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names = 1)
```

Examine the data
```{r}
head(wisc.df)
```
Get rid of the "Diagnosis" column because we won't be needing it
```{r}
wisc.data <- wisc.df[,-1]
```

But store "Diagnosis" as a factor to be used later to check our work
```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
```

> Q1. How many observations are in this dataset?

```{r}
dim(wisc.data)
```
569 observations

> Q2. How many of the observations have a malignant diagnosis?

```{r}
table(diagnosis)
```

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
length(grep("_mean", colnames(wisc.data)))
```
## Principal Component Analysis
Checking column means and standard deviation
```{r}
colMeans(wisc.data)
apply(wisc.data, 2, sd)
```
Perform PCA
```{r}
wisc.pr <- prcomp(wisc.data, scale = T)
```

```{r}
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

From row 2 of the summary above, PC1 accounts for 44.27%

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

From row 3 of the summary above, 3 PCs are required to describe >70%

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

From row 3 of the summary above, 7 PCs are required to describe >90%

## Interpreting PCA Results
```{r}
biplot(wisc.pr)
```
> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

There is way too much information on this plot for it to be understood and to be useful.


Let's make a better plot
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis, xlab = "PC1", ylab = "PC2")
```
> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis, xlab = "PC1", ylab = "PC3")
```
### Using ggplot2 to make another plot
Turn the PCA into a dataframe, add back diagnosis as a column
```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
```
```{r}
library(ggplot2)
```

Make a scatter plot colored by diagnosis
```{r}
ggplot(df, aes(PC1, PC2, col = diagnosis)) +
  geom_point()
```

## Variance Explained
Calculate variance of each component
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```
Calculate the variance explained by each principal component
```{r}
pve <- pr.var / sum(pr.var)
```

Plot variance explained for each principal component
```{r}
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0,1), type = "o")
```

Alternative scree plot of the same data
```{r}
barplot(pve, ylab = "Precent of Variance Explained", names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

### Exploring additional CRAN packages
```{r}
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

## Communicating PCA Results
Loadings: vectors that explain the mapping from the original features to the PC

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation[,1]
```
The loading vector for concave.points_mean is -0.260

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
summary(wisc.pr)
```
You need 5 principal components to explain >80% of the variance

## Hierarchical Clustering
First scale the data
```{r}
data.scaled <- scale(wisc.data)
```

Calculate the distances between all pairs of observations
```{r}
data.dist <- dist(data.scaled)
```

Create hierarchical clustering model
```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```

## Results of HClustering
> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h = 19, col = "red", lty = 2)
```
There are four clusters at about height 19

## Selecting number of clusters
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, h = 19)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```
> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

```{r}
wisc.xclusters <- cutree(wisc.hclust, h = 13)
table(wisc.xclusters, diagnosis)
```
```{r}
wisc.xclusters <- cutree(wisc.hclust, h = 15)
table(wisc.xclusters, diagnosis)
```
```{r}
wisc.xclusters <- cutree(wisc.hclust, h = 18)
table(wisc.xclusters, diagnosis)
```

```{r}
wisc.xclusters <- cutree(wisc.hclust, h = 20)
table(wisc.xclusters, diagnosis)
```

```{r}
wisc.xclusters <- cutree(wisc.hclust, h = 22)
table(wisc.xclusters, diagnosis)
```
```{r}
wisc.xclusters <- cutree(wisc.hclust, h = 25)
table(wisc.xclusters, diagnosis)
```

4 cluster provided some of the best results but using 5 clusters, at h = 18, could be a better decision depending on the data and what you value. The two main benign and malignant groups retain the same values as in the original 4 cluster model but now, the additional three clusters are at least 100% benign or 100% malignant. In the 4 cluster model, one of the additional clusters was a mix of benign (29%) and malignant (71%).

## Using Different Methods
> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

"complete" for reference
```{r}
plot(wisc.hclust)
```

"average" 
```{r}
plot(hclust(data.dist, method = "average"))
```

"single"
```{r}
plot(hclust(data.dist, method = "single"))
```
"ward.D2"
```{r}
plot(hclust(data.dist, method = "ward.D2"))
```
My favorite method was ward.D2 because it was the easiest to read. The symmetry of the plot made the larger clusters much easier to see. Additionally, all branches ended in the same place on a horizontal line which makes more conceptual sense to me.

## K-means Clustering
```{r}
data.scaled <- scale(wisc.data)
wisc.km <- kmeans(data.scaled, centers = 2, nstart = 20)
```
```{r}
table(wisc.km$cluster, diagnosis)
```
> Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?

```{r}
table(wisc.hclust.clusters, wisc.km$cluster)
```
The k means model groups the individuals into two clear groups that can be associated with B or M. This is fewer groups than in the hcluster model that makes the grouping easier to understand. In general though, the two methods corroborate with each other.

## Combining Methods
Does PCA improve or degrade the performance of hierarchical clustering?
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method = "ward.D2")
plot(wisc.pr.hclust)
```

Are these two main branches representative of malignant and benign tumors?
Can cut using h = 80 or k = 2

```{r}
grps <- cutree(wisc.pr.hclust, k = 2)
table(grps)
```
```{r}
table(grps, diagnosis)
```
```{r}
plot(wisc.pr$x[, 1:2], col = grps)
```
```{r}
plot(wisc.pr$x[, 1:2], col = diagnosis)
```
To switch the colors so they are less confusing, turn grps into a factor and reorder the levels
```{r}
g <- as.factor(grps)
levels(g)
```
```{r}
g <- relevel(g,2)
levels(g)
```
Try the plot again
```{r}
plot(wisc.pr$x[, 1:2], col = g)
```
> Q15. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k = 2)
```
```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```
This new combined model does a better job separating the individuals because there are two clear groups, one associated with both B and M. Having four clusters in the previous hclust model was more confusing considering there are only two possible diagnoses.

> Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

```{r}
table(wisc.km$cluster, diagnosis)
```
```{r}
table(wisc.hclust.clusters, diagnosis)
```
The k means model is more similar to the actual diagnoses because there are only two groups whereas the hcluster model has two extra groups that have fewer individuals and therefore are harder to confidently associate with either B or M.
Overall, the two clusterings seem to be very similar, the two main B and M groups are only different by <10 individuals.

## Sensitivity and Specificity
**Sensitivity** : test's ability to correctly detect ill patients with the condition. In this case, the total number of samples in a cluster identified as M divided by the number of known M or TP/(TP+FN)
**Specificity** : test's ability to correctly reject healthy patients without the condition. In this case, number of samples in a cluster identified as B divided by the number of true B or TN/(TN+FP)
Can also look at **accuracy** = #correct / total
```{r}
# for pr hclust combo
(165 + 351)/nrow(wisc.data)
```


> Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

```{r}
hclust.sens <- (165+5+2)/(165+5+2+40)
hclust.spec <- 343/(343+12+2)
hclust.pr.sens <- 188/(188+24)
hclust.pr.spec <- 329/(329+28)
kmeans.sens <- 175/(175+37)
kmeans.spec <- 343/(343+14)
```
```{r}
hclust.sens
hclust.spec
hclust.pr.sens
hclust.pr.spec
kmeans.sens
kmeans.spec
```
**Sensitivity**:
hclust/PCA > K-means > hclust

**Specificity**:
hclust = K-means > hclust/PCA

## Prediction
Take our previous PCA model and apply it to new cancer cell data
```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```
Add these samples to our PCA plot
```{r}
plot(wisc.pr$x[,1:2], col = g)
points(npc[,1], npc[,2], col = "blue", pch = 16, cex = 3)
text(npc[,1], npc[,2], c(1,2), col = "white")
```
> Q18. Which of these new patients should we prioritize for follow up based on your results?

We would prioritize patient 2 because they are grouped in the malignant group.
